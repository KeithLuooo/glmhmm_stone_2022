{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5: Comparing GLM vs. GLM-HMM Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook recreates the figure panels included in Figure 5 of [Bolkan, Stone et al 2021](https://www.biorxiv.org/content/10.1101/2021.07.23.453573v1). It also serves as a useful tutorial notebook for users who are looking to compare GLM vs. GLM-HMM performance for models fit to their own experimental data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general premise of this notebook/figure, in the context of the paper, is that we fit a Bernoulli GLM to our data and realized that a GLM does not provide a great explanation of the data. Instead, we considered a model (called a GLM-HMM) in which the animals' decision-making process could be described by multiple GLMs, each one corresponding to a different internal state or task strategy. After testing how many states best describes the data (see <code>extdatafig7.ipynb</code> for details) we settled on a 3-state GLM-HMM. Below, we'll compare how the 3-state GLM-HMM performs relative to the standard GLM. \n",
    "\n",
    "We will conclude at the end of this notebook that the 3-state GLM-HMM performs better than the GLM, and so we'll stick with that model for all subsequent paper analyses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "### Running Cross-Validation\n",
    "For a few of the figure panels, we're going to train the models on training data and then compare model performance when fit to test data. This requires us to first run cross-validation for both models.\n",
    "####  Import the required code packages and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from glmhmm import glm, glm_hmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data for the indirect pathway cohort\n",
    "x_d2 = np.load('data/indirect_x.npy') # z-scored design matrix\n",
    "y_d2 = np.load('data/indirect_y.npy') # vector of right and left choices for each trial\n",
    "sessions_d2 = np.load('data/indirect_sessions.npy') # vector of session start and stop indices\n",
    "mouseIDs_d2 = np.load('data/indirect_mouseIDs.npy') # vector of mouse IDs for each trial\n",
    "\n",
    "# load the data for the direct pathway cohort\n",
    "x_d1 = np.load('data/direct_x.npy') # z-scored design matrix\n",
    "y_d1 = np.load('data/direct_y.npy') # vector of right and left choices for each trial\n",
    "sessions_d1 = np.load('data/direct_sessions.npy') # vector of session start and stop indices\n",
    "mouseIDs_d1 = np.load('data/direct_mouseIDs.npy') # vector of mouse IDs for each trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split our data into train and test sets. This can be a little tricky to do with real data, as we don't necessarily want to split the data randomly. Instead, we'll want to preserve session structure (as opposed to splitting trials within sessions). Because of individual differences in animals, we'll also want to try to balance the test sets so that they contain approximately the same number of sessions per mouse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glmhmm.utils import splitData\n",
    "\n",
    "## indirect pathway cohort --------------------------------------------------------\n",
    "\n",
    "# initialize as lists since not every test/train set will be exactly the same size\n",
    "x_train_d2, x_test_d2, y_train_d2, y_test_d2, sessions_train_d2, sessions_test_d2, testIx_d2 = [],[],[],[],[],[],[]\n",
    "\n",
    "# specify seeds for splitting the data for reproducibility\n",
    "seeds = [55,38,13,23,103]\n",
    "\n",
    "# split the data\n",
    "for seed in seeds:\n",
    "    trainIx, sessionsTrain, testIx, sessionsTest = splitData(sessions_d2,mouseIDs_d2,testSize=0.2,seed=seed)\n",
    "    x_train_d2.append(x_d2[trainIx,:])\n",
    "    x_test_d2.append(x_d2[testIx,:])\n",
    "    y_train_d2.append(y_d2[trainIx])\n",
    "    y_test_d2.append(y_d2[testIx])\n",
    "    sessions_train_d2.append(sessionsTrain)\n",
    "    sessions_test_d2.append(sessionsTest)\n",
    "    testIx_d2.append(testIx)\n",
    "    \n",
    "## direct pathway cohort --------------------------------------------------------\n",
    "    \n",
    "# initialize as lists since not every test/train set will be exactly the same size\n",
    "x_train_d1, x_test_d1, y_train_d1, y_test_d1, sessions_train_d1, sessions_test_d1, testIx_d1 = [],[],[],[],[],[],[]\n",
    "\n",
    "# specify seeds for splitting the data for reproducibility\n",
    "seeds = [10,66,100,73,200]\n",
    "\n",
    "# split the data\n",
    "for seed in seeds:\n",
    "    trainIx, sessionsTrain, testIx, sessionsTest = splitData(sessions_d1,mouseIDs_d1,testSize=0.2,seed=seed)\n",
    "    x_train_d1.append(x_d1[trainIx,:])\n",
    "    x_test_d1.append(x_d1[testIx,:])\n",
    "    y_train_d1.append(y_d1[trainIx])\n",
    "    y_test_d1.append(y_d1[testIx])\n",
    "    sessions_train_d1.append(sessionsTrain)\n",
    "    sessions_test_d1.append(sessionsTest)\n",
    "    testIx_d1.append(testIx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit GLMs to the training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10 # number of input features\n",
    "C = 2 # number of observation classes\n",
    "folds = 5\n",
    "\n",
    "## indirect pathway cohort --------------------------------------------------------\n",
    "fit_glms_d2 = np.zeros((folds),dtype=object)\n",
    "for i in range(folds):\n",
    "    N = x_train_d2[i].shape[0] \n",
    "    fit_glms_d2[i] = glm.GLM(N,M,C,observations=\"bernoulli\")\n",
    "    w_init = fit_glms_d2[i].init_weights()\n",
    "    results = fit_glms_d2[i].fit(x_train_d2[i],w_init,y_train_d2[i],compHess=False)\n",
    "    \n",
    "## direct pathway cohort --------------------------------------------------------\n",
    "fit_glms_d1 = np.zeros((folds),dtype=object)\n",
    "for i in range(folds):\n",
    "    N = x_train_d1[i].shape[0] \n",
    "    fit_glms_d1[i] = glm.GLM(N,M,C,observations=\"bernoulli\")\n",
    "    w_init = fit_glms_d1[i].init_weights()\n",
    "    results = fit_glms_d1[i].fit(x_train_d1[i],w_init,y_train_d1[i],compHess=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit GLM-HMMs to the training sets\n",
    "The cell below will take about 20 hours to run (~2 hours per fold x 5 folds x 2 datasets) but of course you can speed this up by putting the code below into a python script and running it for each fold (and initialization) in parallel.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glmhmm.utils import find_best_fit\n",
    "\n",
    "K = 3\n",
    "inits = 20 # set the number of initializations\n",
    "\n",
    "## indirect pathway cohort --------------------------------------------------------\n",
    "best_fit_GLMHMMs_d2 = np.zeros((folds),dtype=object)\n",
    "for j in range(folds):\n",
    "    # store values for each initialization\n",
    "    lls_all = np.zeros((inits,250))\n",
    "    real_GLMHMMs = np.zeros((inits),dtype=object)\n",
    "\n",
    "    # fit the model for each initialization\n",
    "    N = x_train_d2[j].shape[0]\n",
    "    for i in range(inits): \n",
    "        real_GLMHMMs[i] = glm_hmm.GLMHMM(N,M,C,K,observations=\"bernoulli\")\n",
    "        A_init,w_init,_ = real_GLMHMMs[i].generate_params(weights=['GLM',-0.2,1.2,x_train_d2[j],y_train_d2[j],1])                   \n",
    "        lls_all[i],_,_,_ = real_GLMHMMs[i].fit(y_train_d2[j],x_train_d2[j],A_init,w_init,sess=sessions_train_d2[j]) \n",
    "        \n",
    "    # store results from best fit\n",
    "    bestix = find_best_fit(lls_all)\n",
    "    best_fit_GLMHMMs_d2[j] = real_GLMHMMs[bestix]\n",
    "    \n",
    "## direct pathway cohort --------------------------------------------------------\n",
    "best_fit_GLMHMMs_d1 = np.zeros((folds),dtype=object)\n",
    "for j in range(folds):\n",
    "    # store values for each initialization\n",
    "    lls_all = np.zeros((inits,250))\n",
    "    real_GLMHMMs = np.zeros((inits),dtype=object)\n",
    "\n",
    "    # fit the model for each initialization\n",
    "    N = x_train_d1[j].shape[0]\n",
    "    for i in range(inits): \n",
    "        real_GLMHMMs[i] = glm_hmm.GLMHMM(N,M,C,K,observations=\"bernoulli\")\n",
    "        A_init,w_init,_ = real_GLMHMMs[i].generate_params(weights=['GLM',-0.2,1.2,x_train_d1[j],y_train_d1[j],1])                   \n",
    "        lls_all[i],_,_,_ = real_GLMHMMs[i].fit(y_train_d1[j],x_train_d1[j],A_init,w_init,sess=sessions_train_d1[j]) \n",
    "        \n",
    "    # store results from best fit\n",
    "    bestix = find_best_fit(lls_all)\n",
    "    best_fit_GLMHMMs_d1[j] = real_GLMHMMs[bestix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll compute the test loglikelihoods for the fit GLM and GLM-HMM models for each mouse and plot the results to see how much improvement in performance we see for the GLM-HMM over the GLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glmhmm.analysis import compare_LL_GLMvsGLMHMM\n",
    "\n",
    "unique_mouseIDs = np.unique(mouseIDs_d2)\n",
    "numMice = len(uniqueMouseIDs)\n",
    "test_lls = np.zeros((folds,numMice,2))\n",
    "for j in range(folds):\n",
    "    test_mouseIDs = mouseIDs_d2[testIx_d2[j]]\n",
    "    for mouseID in unique_mouseIDs:\n",
    "        test_mouse_ix = np.where(testmouseIDs == mouseID)\n",
    "        test_lls[i,j] = compare_LL_GLMvsGLMHMM(fit_glm_d2[j],best_fit_GLMHMMs_d2[j],x[test_mouse_ix,:],\n",
    "                                               y[test_mouse_ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
